{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: TFRT_CPU_0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d7/pbk8bck12h7bh19pktsg__3w0000gn/T/ipykernel_85237/2725606648.py:15: DeprecationWarning: `set_matplotlib_formats` is deprecated since IPython 7.23, directly use `matplotlib_inline.backend_inline.set_matplotlib_formats()`\n",
      "  set_matplotlib_formats('svg', 'pdf') # For export\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Similar to the documentation, i'll be implementing different attention mechanisms\n",
    "# Simple, Multi, 2Simplical Attention\n",
    "## Standard libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import math\n",
    "import json\n",
    "from functools import partial\n",
    "\n",
    "## Imports for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "plt.set_cmap('cividis')\n",
    "%matplotlib inline\n",
    "from IPython.display import set_matplotlib_formats\n",
    "set_matplotlib_formats('svg', 'pdf') # For export\n",
    "from matplotlib.colors import to_rgb\n",
    "import matplotlib\n",
    "matplotlib.rcParams['lines.linewidth'] = 2.0\n",
    "import seaborn as sns\n",
    "sns.reset_orig()\n",
    "\n",
    "## tqdm for loading bars\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "## To run JAX on TPU in Google Colab, uncomment the two lines below\n",
    "# import jax.tools.colab_tpu\n",
    "# jax.tools.colab_tpu.setup_tpu()\n",
    "\n",
    "## JAX\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import random\n",
    "# Seeding for random operations\n",
    "main_rng = random.PRNGKey(42)\n",
    "\n",
    "## Flax (NN in JAX)\n",
    "from flax import linen as nn\n",
    "from flax.training import train_state, checkpoints\n",
    "\n",
    "## Optax (Optimizers in JAX)\n",
    "\n",
    "import optax\n",
    "\n",
    "## PyTorch\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import CIFAR100\n",
    "\n",
    "# Path to the folder where the datasets are/should be downloaded (e.g. CIFAR10)\n",
    "DATASET_PATH = \"/Users/atreyasridharan/miniforge3/bin/JAX_learning/data\"\n",
    "# Path to the folder where the pretrained models are saved\n",
    "CHECKPOINT_PATH = \"/Users/atreyasridharan/miniforge3/bin/JAX_learning/tutorial6_jax\"\n",
    "\n",
    "print(\"Device:\", jax.devices()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://raw.githubusercontent.com/phlippe/saved_models/main/JAX/tutorial6/ReverseTask.ckpt...\n",
      "Downloading https://raw.githubusercontent.com/phlippe/saved_models/main/JAX/tutorial6/SetAnomalyTask.ckpt...\n"
     ]
    }
   ],
   "source": [
    "#Don't really need this, just doing it to check model benchmarks\n",
    "\n",
    "import urllib.request\n",
    "from urllib.error import HTTPError\n",
    "# Github URL where saved models are stored for this tutorial\n",
    "base_url = \"https://raw.githubusercontent.com/phlippe/saved_models/main/JAX/tutorial6/\"\n",
    "# Files to download\n",
    "pretrained_files = [\"ReverseTask.ckpt\", \"SetAnomalyTask.ckpt\"]\n",
    "\n",
    "# Create checkpoint path if it doesn't exist yet\n",
    "os.makedirs(CHECKPOINT_PATH, exist_ok=True)\n",
    "\n",
    "# For each file, check whether it already exists. If not, try downloading it.\n",
    "for file_name in pretrained_files:\n",
    "    file_path = os.path.join(CHECKPOINT_PATH, file_name)\n",
    "    if \"/\" in file_name:\n",
    "        os.makedirs(file_path.rsplit(\"/\",1)[0], exist_ok=True)\n",
    "    if not os.path.isfile(file_path):\n",
    "        file_url = base_url + file_name\n",
    "        print(f\"Downloading {file_url}...\")\n",
    "        try:\n",
    "            urllib.request.urlretrieve(file_url, file_path)\n",
    "        except HTTPError as e:\n",
    "            print(\"Something went wrong. Please try to download the file from the GDrive folder, or contact the author with the full output including the following error:\\n\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Single attention\n",
    "\n",
    "def scaled_dot_product(q,k,v, mask=None):\n",
    "    d_k = q.shape[-1]\n",
    "    attn_logits = jnp.matmul(q, jnp.swapaxes(k, -2, -1))\n",
    "    attn_logits = attn_logits / math.sqrt(d_k)\n",
    "    if mask is not None:\n",
    "        attn_logits = jnp.where(mask == 0, -9e15, attn_logits)\n",
    "    attention = nn.softmax(attn_logits, axis=-1)\n",
    "    values = jnp.matmul(attention,v)\n",
    "    return values, attention\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q\n",
      " [[ 0.60576403  0.7990441 ]\n",
      " [-0.908927   -0.63525754]\n",
      " [-1.2226585  -0.83226097]]\n",
      "K\n",
      " [[-0.47417238 -1.2504351 ]\n",
      " [-0.17678244 -0.04917514]\n",
      " [-0.41177532 -0.39363015]]\n",
      "V\n",
      " [[ 1.3116323   0.21555556]\n",
      " [ 0.41164538 -0.28955024]\n",
      " [-0.96516913  0.4492738 ]]\n",
      "Values\n",
      " [[0.12734914 0.06441191]\n",
      " [0.4115729  0.17320421]\n",
      " [0.46902645 0.1854193 ]]\n",
      "Attention\n",
      " [[0.20383833 0.4564296  0.33973208]\n",
      " [0.46830934 0.2255167  0.30617398]\n",
      " [0.51187545 0.19520193 0.29292265]]\n"
     ]
    }
   ],
   "source": [
    "seq_len, d_k = 3 , 2\n",
    "main_rng ,rand1 = random.split(main_rng)\n",
    "qkv = random.normal(rand1, (3, seq_len, d_k))\n",
    "q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "values, attention = scaled_dot_product(q, k, v)\n",
    "print(\"Q\\n\", q)\n",
    "print(\"K\\n\", k)\n",
    "print(\"V\\n\", v)\n",
    "print(\"Values\\n\", values)\n",
    "print(\"Attention\\n\", attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper func for diff mask shapes\n",
    "def expand_mask(mask):\n",
    "    assert mask.ndim >= 2\n",
    "    if mask.ndim == 3:\n",
    "        mask = mask.unsqueeze(1)\n",
    "    while mask.ndim < 4:\n",
    "        mask = mask.unsqueeze(0)\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "\n",
    "\n",
    "class MultiheadAttention(nn.Module):\n",
    "    embed_dim : int\n",
    "    num_heads : int\n",
    "\n",
    "    def setup(self):\n",
    "        self.qkv_proj = nn.Dense(3*self.embed_dim,\n",
    "                                 kernel_init=nn.initializers.xavier_uniform(),\n",
    "                                 bias_init= nn.initializers.zeros)\n",
    "        \n",
    "        self.o_proj = nn.Dense(self.embed_dim,\n",
    "                               kernel_init= nn.initializers.xavier_uniform(),\n",
    "                               bias_init= nn.initializers.zeros)\n",
    "        \n",
    "    def __call__(self, x, mask=None):\n",
    "        batch_size, seq_length, embed_dim = x.shape\n",
    "        if mask is not None:\n",
    "            mask = expand_mask(mask)\n",
    "        qkv = self.qkv_proj(x)\n",
    "\n",
    "        qkv = qkv.reshape(batch_size,seq_length, self.num_heads, -1)\n",
    "        qkv = qkv.transpose(0,2,1,3)\n",
    "        q,k,v = jnp.array_split(qkv, 3, axis=-1)\n",
    "\n",
    "        #Determine value outputs\n",
    "        values, attention = scaled_dot_product(q,k,v, mask=mask)\n",
    "        values = values.transpose(0,2,1,3)\n",
    "        values = values.reshape(batch_size, seq_length, embed_dim)\n",
    "        o = self.o_proj(values)\n",
    "\n",
    "        return o,attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out (3, 16, 128) Attention (3, 4, 16, 16)\n"
     ]
    }
   ],
   "source": [
    "main_rng, x_rng = random.split(main_rng)\n",
    "x = random.normal(x_rng, (3, 16, 128))\n",
    "# Create attention\n",
    "mh_attn = MultiheadAttention(embed_dim=128, num_heads=4)\n",
    "# Initialize parameters of attention with random key and inputs\n",
    "main_rng, init_rng = random.split(main_rng)\n",
    "params = mh_attn.init(init_rng, x)['params']\n",
    "# Apply attention with parameters on the inputs\n",
    "out, attn = mh_attn.apply({'params': params}, x)\n",
    "print('Out', out.shape, 'Attention', attn.shape)\n",
    "\n",
    "del mh_attn, params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encoder\n",
    "from typing import Any\n",
    "\n",
    "\n",
    "class EncoderBlock(nn.Module):\n",
    "    input_dim : int\n",
    "    num_heads : int\n",
    "    dim_feedforward : int\n",
    "    dropout_prob : float\n",
    "\n",
    "    def setup(self):\n",
    "        self.self_attn = MultiheadAttention(embed_dim=self.input_dim, num_heads=self.num_heads)\n",
    "\n",
    "        self.linear = [\n",
    "            nn.Dense(self.dim_feedforward),\n",
    "            nn.Dropout(self.dropout_prob),\n",
    "            nn.relu,\n",
    "            nn.Dense(self.input_dim)\n",
    "        ]\n",
    "\n",
    "        self.norm1 = nn.LayerNorm()\n",
    "        self.norm2 = nn.LayerNorm()\n",
    "        self.dropout = nn.Dropout(self.dropout_prob)\n",
    "\n",
    "    def __call__(self, x, mask=None, train = True):\n",
    "        attn_out, _ = self.self_attn(x, mask=mask)\n",
    "        x = x + self.dropout(attn_out, deterministic=not train)\n",
    "        x = self.norm1(x)\n",
    "\n",
    "        linear_out = x\n",
    "        for l in self.linear:\n",
    "            linear_out = l(linear_out) if not isinstance(l, nn.Dropout) else l(linear_out, deterministic= not train)\n",
    "        x = x + self.dropout(linear_out, deterministic= not train)\n",
    "        x = self.norm2(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    num_layers : int\n",
    "    input_dim : int\n",
    "    num_heads : int\n",
    "    dim_feedforward : int\n",
    "    dropout_prob : float\n",
    "\n",
    "    def setup(self):\n",
    "        self.layers = [EncoderBlock(self.input_dim, self.num_heads, self.dim_feedforward, self.dropout_prob) for _ in range(self.num_layers)]\n",
    "\n",
    "    def __call__(self, x, mask=None, train=True):\n",
    "        for l in self.layers:\n",
    "            x = l(x, mask=mask, train=train)\n",
    "        return x\n",
    "\n",
    "    def get_attention_maps(self, x, mask=None, train=True):\n",
    "        # A function to return the attention maps within the model for a single application\n",
    "        # Used for visualization purpose later\n",
    "        attention_maps = []\n",
    "        for l in self.layers:\n",
    "            _, attn_map = l.self_attn(x, mask=mask)\n",
    "            attention_maps.append(attn_map)\n",
    "            x = l(x, mask=mask, train=train)\n",
    "        return attention_maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RoPE (Not in the tutorial)\n",
    "\n",
    "class RotaryPositionalEmbedding(nn.Module):\n",
    "    d_model : int\n",
    "    max_seq_len : int\n",
    "\n",
    "    def setup(self):\n",
    "        self.rotation_matrix = jnp.zeros((self.d_model, self.d_model), dtype=jnp.float32)\n",
    "        for i  in self.d_model:\n",
    "            for j in self.d_model:\n",
    "                rotation_matrix = rotation_matrix.at[i,j].set(np.cos(i * j * 0.01))\n",
    "        \n",
    "        self.rotation_matrix = rotation_matrix\n",
    "\n",
    "        self.positional_embeddings = jnp.zeros((self.max_seq_len, self.d_model), dtype=jnp.float32)\n",
    "        for i in self.max_seq_len:\n",
    "            for j in self.d_model:\n",
    "                positional_embeddings = positional_embeddings.at[i,j].set(jnp.cos(i * j * 0.01))\n",
    "        \n",
    "        positional_embeddings = self.positional_embeddings\n",
    "        \n",
    "    def call(self, x, mask=None, train=True):\n",
    "        x += self.positional_embeddings\n",
    "        x = jnp.matmul(x, self.rotation_matrix)\n",
    "\n",
    "        return x\n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'int' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m encod_block \u001b[38;5;241m=\u001b[39m RotaryPositionalEmbedding(d_model\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m48\u001b[39m, max_seq_len\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m96\u001b[39m)\u001b[38;5;241m.\u001b[39mbind({})\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# Obtain positional encodings as numpy array\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m pe \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mdevice_get(\u001b[43mencod_block\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpositional_embeddings\u001b[49m\u001b[38;5;241m.\u001b[39msqueeze()\u001b[38;5;241m.\u001b[39mT)\n\u001b[1;32m      5\u001b[0m fig, ax \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39msubplots(nrows\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, ncols\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m8\u001b[39m,\u001b[38;5;241m3\u001b[39m))\n\u001b[1;32m      6\u001b[0m pos \u001b[38;5;241m=\u001b[39m ax\u001b[38;5;241m.\u001b[39mimshow(pe, cmap\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRdGy\u001b[39m\u001b[38;5;124m\"\u001b[39m, extent\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m1\u001b[39m,pe\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m,pe\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m))\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.10/site-packages/flax/linen/module.py:1304\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m _UNDEFINED_COPY_PICKLE_METHODS:\n\u001b[1;32m   1303\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m()\n\u001b[0;32m-> 1304\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_setup\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m:\n\u001b[1;32m   1306\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m[name]\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.10/site-packages/flax/linen/module.py:1494\u001b[0m, in \u001b[0;36mModule._try_setup\u001b[0;34m(self, shallow)\u001b[0m\n\u001b[1;32m   1492\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_register_submodules(field\u001b[38;5;241m.\u001b[39mname, \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, field\u001b[38;5;241m.\u001b[39mname))\n\u001b[1;32m   1493\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m shallow:\n\u001b[0;32m-> 1494\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msetup\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1495\u001b[0m   \u001b[38;5;66;03m# create NonTransparent Modules\u001b[39;00m\n\u001b[1;32m   1496\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compact_name_scope_modules \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m   1497\u001b[0m     name: CompactNameScope(\n\u001b[1;32m   1498\u001b[0m       \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m), name)\u001b[38;5;241m.\u001b[39minner_fun, \u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[38;5;28mself\u001b[39m, name\u001b[38;5;241m=\u001b[39mname\n\u001b[1;32m   1499\u001b[0m     )\n\u001b[1;32m   1500\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compact_name_scope_methods\n\u001b[1;32m   1501\u001b[0m   }\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.10/site-packages/flax/linen/module.py:699\u001b[0m, in \u001b[0;36mwrap_method_once.<locals>.wrapped_module_method\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    697\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m args \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(args[\u001b[38;5;241m0\u001b[39m], Module):\n\u001b[1;32m    698\u001b[0m   \u001b[38;5;28mself\u001b[39m, args \u001b[38;5;241m=\u001b[39m args[\u001b[38;5;241m0\u001b[39m], args[\u001b[38;5;241m1\u001b[39m:]\n\u001b[0;32m--> 699\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_wrapped_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    700\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    701\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fun(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.10/site-packages/flax/linen/module.py:1216\u001b[0m, in \u001b[0;36mModule._call_wrapped_method\u001b[0;34m(self, fun, args, kwargs)\u001b[0m\n\u001b[1;32m   1214\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _use_named_call:\n\u001b[1;32m   1215\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m jax\u001b[38;5;241m.\u001b[39mnamed_scope(_derive_profiling_name(\u001b[38;5;28mself\u001b[39m, fun)):\n\u001b[0;32m-> 1216\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[43mrun_fun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1217\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1218\u001b[0m   y \u001b[38;5;241m=\u001b[39m run_fun(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "Cell \u001b[0;32mIn[36], line 9\u001b[0m, in \u001b[0;36mRotaryPositionalEmbedding.setup\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msetup\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrotation_matrix \u001b[38;5;241m=\u001b[39m jnp\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39md_model, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39md_model), dtype\u001b[38;5;241m=\u001b[39mjnp\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m----> 9\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i  \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39md_model:\n\u001b[1;32m     10\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39md_model:\n\u001b[1;32m     11\u001b[0m             rotation_matrix \u001b[38;5;241m=\u001b[39m rotation_matrix\u001b[38;5;241m.\u001b[39mat[i,j]\u001b[38;5;241m.\u001b[39mset(np\u001b[38;5;241m.\u001b[39mcos(i \u001b[38;5;241m*\u001b[39m j \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m0.01\u001b[39m))\n",
      "\u001b[0;31mTypeError\u001b[0m: 'int' object is not iterable"
     ]
    }
   ],
   "source": [
    "encod_block = RotaryPositionalEmbedding(d_model=48, max_seq_len=96).bind({})\n",
    "# Obtain positional encodings as numpy array\n",
    "pe = jax.device_get(encod_block.positional_embeddings.squeeze().T)\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(8,3))\n",
    "pos = ax.imshow(pe, cmap=\"RdGy\", extent=(1,pe.shape[1]+1,pe.shape[0]+1,1))\n",
    "fig.colorbar(pos, ax=ax)\n",
    "ax.set_xlabel(\"Position in sequence\")\n",
    "ax.set_ylabel(\"Hidden dimension\")\n",
    "ax.set_title(\"Positional encoding over hidden dimensions\")\n",
    "ax.set_xticks([1]+[i*10 for i in range(1,1+pe.shape[1]//10)])\n",
    "ax.set_yticks([1]+[i*10 for i in range(1,1+pe.shape[0]//10)])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
