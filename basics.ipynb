{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using jax 0.6.2\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "\n",
    "print(\"using jax\", jax.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[0 1 2 3 4 5]\n"
     ]
    }
   ],
   "source": [
    "# creating arrays\n",
    "\n",
    "a = jnp.zeros((2,5), dtype= jnp.float32)\n",
    "print(a)\n",
    "\n",
    "b = jnp.arange(6) #creates an array of 6 elements (indexing standard at 0)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Array: [0 1 2 3 4 5]\n",
      "Changed Array: [1 1 2 3 4 5]\n"
     ]
    }
   ],
   "source": [
    "#Immutable tensors\n",
    "\n",
    "#cannot change b[0] = 1 have to set like below (Pure function type)\n",
    "b_new = b.at[0].set(1)\n",
    "print('Original Array:', b)\n",
    "print('Changed Array:', b_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input [0. 1. 2.]\n",
      "Output 12.666667\n"
     ]
    }
   ],
   "source": [
    "#Want to write main code of JAX in functions that only affect output\n",
    "\n",
    "def simple_graph(x):\n",
    "    x = x+2\n",
    "    x = x**2\n",
    "    x = x+3\n",
    "    y = x.mean()\n",
    "    return y\n",
    "\n",
    "inp = jnp.arange(3, dtype=jnp.float32)\n",
    "print('Input', inp)\n",
    "print('Output', simple_graph(inp))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{ \u001b[34;1mlambda \u001b[39;22m; a\u001b[35m:f32[3]\u001b[39m. \u001b[34;1mlet\n",
       "    \u001b[39;22mb\u001b[35m:f32[3]\u001b[39m = add a 2.0:f32[]\n",
       "    c\u001b[35m:f32[3]\u001b[39m = integer_pow[y=2] b\n",
       "    d\u001b[35m:f32[3]\u001b[39m = add c 3.0:f32[]\n",
       "    e\u001b[35m:f32[]\u001b[39m = reduce_sum[axes=(0,)] d\n",
       "    f\u001b[35m:f32[]\u001b[39m = div e 3.0:f32[]\n",
       "  \u001b[34;1min \u001b[39;22m(f,) }"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jax.make_jaxpr(simple_graph)(inp)\n",
    "\n",
    "#jaxpr representation\n",
    "#jaxpr ::= {\n",
    "  #  lambda Var*; Var+.\n",
    "    #let Eqn*\n",
    "   # in [Expr+]\n",
    "#} \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient: [1.3333334 2.        2.6666667]\n"
     ]
    }
   ],
   "source": [
    "#Automatic Differentiation\n",
    "\n",
    "grad_function = jax.grad(simple_graph)\n",
    "gradients = grad_function(inp)\n",
    "print('Gradient:', gradients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad function <function simple_graph at 0x1179e5ea0>\n"
     ]
    }
   ],
   "source": [
    "print('grad function', grad_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Array(12.666667, dtype=float32),\n",
       " Array([1.3333334, 2.       , 2.6666667], dtype=float32))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Return actual output along with gradients\n",
    "val_grad_function = jax.value_and_grad(simple_graph)\n",
    "val_grad_function(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting flax\n",
      "  Downloading flax-0.10.7-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: jax>=0.6.0 in /Users/atreyasridharan/miniforge3/lib/python3.10/site-packages (from flax) (0.6.2)\n",
      "Collecting msgpack (from flax)\n",
      "  Downloading msgpack-1.1.1-cp310-cp310-macosx_11_0_arm64.whl.metadata (8.4 kB)\n",
      "Collecting optax (from flax)\n",
      "  Downloading optax-0.2.5-py3-none-any.whl.metadata (7.5 kB)\n",
      "Collecting orbax-checkpoint (from flax)\n",
      "  Downloading orbax_checkpoint-0.11.20-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting tensorstore (from flax)\n",
      "  Downloading tensorstore-0.1.76-cp310-cp310-macosx_11_0_arm64.whl.metadata (21 kB)\n",
      "Collecting rich>=11.1 (from flax)\n",
      "  Downloading rich-14.1.0-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: typing_extensions>=4.2 in /Users/atreyasridharan/miniforge3/lib/python3.10/site-packages (from flax) (4.7.1)\n",
      "Requirement already satisfied: PyYAML>=5.4.1 in /Users/atreyasridharan/miniforge3/lib/python3.10/site-packages (from flax) (6.0.1)\n",
      "Collecting treescope>=0.1.7 (from flax)\n",
      "  Downloading treescope-0.1.9-py3-none-any.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: jaxlib<=0.6.2,>=0.6.2 in /Users/atreyasridharan/miniforge3/lib/python3.10/site-packages (from jax>=0.6.0->flax) (0.6.2)\n",
      "Requirement already satisfied: ml_dtypes>=0.5.0 in /Users/atreyasridharan/miniforge3/lib/python3.10/site-packages (from jax>=0.6.0->flax) (0.5.3)\n",
      "Requirement already satisfied: numpy>=1.26 in /Users/atreyasridharan/miniforge3/lib/python3.10/site-packages (from jax>=0.6.0->flax) (1.26.0)\n",
      "Requirement already satisfied: opt_einsum in /Users/atreyasridharan/miniforge3/lib/python3.10/site-packages (from jax>=0.6.0->flax) (3.4.0)\n",
      "Requirement already satisfied: scipy>=1.12 in /Users/atreyasridharan/miniforge3/lib/python3.10/site-packages (from jax>=0.6.0->flax) (1.15.3)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich>=11.1->flax)\n",
      "  Downloading markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/atreyasridharan/miniforge3/lib/python3.10/site-packages (from rich>=11.1->flax) (2.16.1)\n",
      "Collecting absl-py>=0.7.1 (from optax->flax)\n",
      "  Downloading absl_py-2.3.1-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting chex>=0.1.87 (from optax->flax)\n",
      "  Downloading chex-0.1.90-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting etils[epath,epy] (from orbax-checkpoint->flax)\n",
      "  Downloading etils-1.13.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: nest_asyncio in /Users/atreyasridharan/miniforge3/lib/python3.10/site-packages (from orbax-checkpoint->flax) (1.5.6)\n",
      "Collecting aiofiles (from orbax-checkpoint->flax)\n",
      "  Downloading aiofiles-24.1.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting protobuf (from orbax-checkpoint->flax)\n",
      "  Downloading protobuf-6.31.1-cp39-abi3-macosx_10_9_universal2.whl.metadata (593 bytes)\n",
      "Collecting humanize (from orbax-checkpoint->flax)\n",
      "  Downloading humanize-4.12.3-py3-none-any.whl.metadata (7.8 kB)\n",
      "Collecting simplejson>=3.16.0 (from orbax-checkpoint->flax)\n",
      "  Downloading simplejson-3.20.1-cp310-cp310-macosx_11_0_arm64.whl.metadata (3.3 kB)\n",
      "Requirement already satisfied: toolz>=0.9.0 in /Users/atreyasridharan/miniforge3/lib/python3.10/site-packages (from chex>=0.1.87->optax->flax) (0.12.0)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich>=11.1->flax)\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting fsspec (from etils[epath,epy]->orbax-checkpoint->flax)\n",
      "  Downloading fsspec-2025.7.0-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: importlib_resources in /Users/atreyasridharan/miniforge3/lib/python3.10/site-packages (from etils[epath,epy]->orbax-checkpoint->flax) (6.0.1)\n",
      "Requirement already satisfied: zipp in /Users/atreyasridharan/miniforge3/lib/python3.10/site-packages (from etils[epath,epy]->orbax-checkpoint->flax) (3.16.2)\n",
      "Downloading flax-0.10.7-py3-none-any.whl (456 kB)\n",
      "Downloading rich-14.1.0-py3-none-any.whl (243 kB)\n",
      "Downloading treescope-0.1.9-py3-none-any.whl (182 kB)\n",
      "Downloading msgpack-1.1.1-cp310-cp310-macosx_11_0_arm64.whl (78 kB)\n",
      "Downloading optax-0.2.5-py3-none-any.whl (354 kB)\n",
      "Downloading orbax_checkpoint-0.11.20-py3-none-any.whl (510 kB)\n",
      "Downloading tensorstore-0.1.76-cp310-cp310-macosx_11_0_arm64.whl (13.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.6/13.6 MB\u001b[0m \u001b[31m29.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading absl_py-2.3.1-py3-none-any.whl (135 kB)\n",
      "Downloading chex-0.1.90-py3-none-any.whl (101 kB)\n",
      "Downloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Downloading simplejson-3.20.1-cp310-cp310-macosx_11_0_arm64.whl (75 kB)\n",
      "Downloading aiofiles-24.1.0-py3-none-any.whl (15 kB)\n",
      "Downloading humanize-4.12.3-py3-none-any.whl (128 kB)\n",
      "Downloading protobuf-6.31.1-cp39-abi3-macosx_10_9_universal2.whl (425 kB)\n",
      "Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Downloading etils-1.13.0-py3-none-any.whl (170 kB)\n",
      "Downloading fsspec-2025.7.0-py3-none-any.whl (199 kB)\n",
      "Installing collected packages: treescope, simplejson, protobuf, msgpack, mdurl, humanize, fsspec, etils, aiofiles, absl-py, tensorstore, markdown-it-py, rich, orbax-checkpoint, chex, optax, flax\n",
      "Successfully installed absl-py-2.3.1 aiofiles-24.1.0 chex-0.1.90 etils-1.13.0 flax-0.10.7 fsspec-2025.7.0 humanize-4.12.3 markdown-it-py-3.0.0 mdurl-0.1.2 msgpack-1.1.1 optax-0.2.5 orbax-checkpoint-0.11.20 protobuf-6.31.1 rich-14.1.0 simplejson-3.20.1 tensorstore-0.1.76 treescope-0.1.9\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#Building XOR Classifier\n",
    "%pip install flax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import flax\n",
    "from flax import linen as nn\n",
    "\n",
    "#Flax uses lazy initialization\n",
    "\n",
    "class MyModule(nn.Module):\n",
    "    # Some dataclass attributes, like hidden dimension, number of layers, etc. of the form:\n",
    "    # varname : vartype\n",
    "\n",
    "    def setup(self):\n",
    "        # Flax uses \"lazy\" initialization. This function is called once before you\n",
    "        # call the model, or try to access attributes. In here, define your submodules etc.\n",
    "        pass\n",
    "\n",
    "    def __call__(self, x):\n",
    "        # Function for performing the calculation of the module.\n",
    "        pass\n",
    "\n",
    "#Parameters are kept inside the pytree\n",
    "#No need to define like Pytorch __init__()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Simple classifier \n",
    "class SimpleClassifier(nn.Module):\n",
    "    num_hidden : int\n",
    "    num_output : int\n",
    "\n",
    "    def setup(self):\n",
    "        self.linear1 = nn.Dense(features=self.num_hidden)\n",
    "        self.linear2 = nn.Dense(features=self.num_output)\n",
    "\n",
    "    def __call__(self,x):\n",
    "        x = self.linear1(x)\n",
    "        x = nn.tanh(x)\n",
    "        x = self.linear2(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instead of explicitly layers in the setup function\n",
    "#Use nn.compact to call in the __call__ func\n",
    "\n",
    "class SimpleClassifier(nn.Module):\n",
    "    num_hiddens : int\n",
    "    num_outputs : int\n",
    "\n",
    "    @nn.compact #Tells flax to look for defined submodules\n",
    "    def __call__(self,x):\n",
    "        #Perform calc while defining necessary layers\n",
    "        x = nn.Dense(features=self.num_hiddens)\n",
    "        x = nn.tanh(x)\n",
    "        x = nn.Dense(features=self.num_outputs)\n",
    "        return x\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
